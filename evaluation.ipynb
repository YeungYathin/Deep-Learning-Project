{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86fa9fe2-df42-421c-8e21-6ad4bc1c6a27",
   "metadata": {},
   "source": [
    "# Evaluate text to image generation\n",
    "\n",
    "Build a test captions list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "833c1ef3-1f9d-4cad-86f4-6c1ad8b85f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of captions: 2500\n",
      "Example caption: A man is in a kitchen making pizzas.\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "import clip\n",
    "from model import CVAE\n",
    "\n",
    "# Load the test dataset\n",
    "test_annotations_file = \"autodl-tmp/annotations/captions_val2017.json\"\n",
    "test_coco = COCO(test_annotations_file)\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 128\n",
    "condition_dim = 512\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "subset_fraction = 0.5  # Fraction of the dataset to evaluate\n",
    "\n",
    "# Extract test captions\n",
    "test_captions = []\n",
    "all_image_ids = list(test_coco.imgs.keys())\n",
    "subset_size = int(len(all_image_ids) * subset_fraction)\n",
    "image_ids = all_image_ids[:subset_size]\n",
    "\n",
    "for img_id in image_ids:\n",
    "    ann_ids = test_coco.getAnnIds(imgIds=img_id)\n",
    "    annotations = test_coco.loadAnns(ann_ids)\n",
    "    caption = annotations[0]['caption']\n",
    "    test_captions.append(caption)\n",
    "\n",
    "print(f\"Number of captions: {len(test_captions)}\")\n",
    "print(f\"Example caption: {test_captions[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7256160-7f21-46ae-803a-0a512278b4aa",
   "metadata": {},
   "source": [
    "Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "64cef857-351c-45db-8948-efad8a8a4898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from model import CVAE\n",
    "import torch\n",
    "\n",
    "latent_dim = 128\n",
    "condition_dim = 512  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CVAE(condition_dim=condition_dim, latent_dim=latent_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"model_250.pth\"))\n",
    "model.eval()  \n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f93b4529-0610-4bf2-958f-76863a193f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP model loaded successfully!\n",
      "Average CLIP Similarity: 0.2002\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP model\n",
    "model_clip, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "print(\"CLIP model loaded successfully!\")\n",
    "\n",
    "# CLIP Similarity Evaluation Function\n",
    "def clip_similarity(model, captions, device, num_samples=10):\n",
    "    \"\"\"\n",
    "    Evaluates the CLIP similarity between generated images and input captions.\n",
    "\n",
    "    Args:\n",
    "        model: CVAE model.\n",
    "        captions: List of text captions for evaluation.\n",
    "        device: Device to perform computations.\n",
    "        num_samples: Number of samples to evaluate (default: 10).\n",
    "\n",
    "    Returns:\n",
    "        Average CLIP similarity score.\n",
    "    \"\"\"\n",
    "    total_similarity = 0.0\n",
    "    num_evaluated = min(len(captions), num_samples)  # Limit the number of samples\n",
    "\n",
    "    for i in range(num_evaluated):\n",
    "        # Get the text caption\n",
    "        caption = captions[i]\n",
    "        text_token = clip.tokenize([caption]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Encode text into CLIP feature space\n",
    "            text_feature = model_clip.encode_text(text_token)\n",
    "\n",
    "            # Generate latent vector and decode an image\n",
    "            z = torch.randn(1, latent_dim).to(device)  # Random latent vector\n",
    "            generated_image = model.decoder(z, text_feature).view(1, 3, 224, 224).to(device)\n",
    "\n",
    "            # Encode the generated image into CLIP feature space\n",
    "            image_feature = model_clip.encode_image(generated_image)\n",
    "\n",
    "            # Compute cosine similarity\n",
    "            similarity = torch.nn.functional.cosine_similarity(text_feature, image_feature, dim=-1).item()\n",
    "            total_similarity += similarity\n",
    "\n",
    "    # Return the average similarity score\n",
    "    return total_similarity / num_evaluated\n",
    "\n",
    "# Evaluate the model\n",
    "average_similarity = clip_similarity(model, test_captions, device, num_samples=2000)\n",
    "print(f\"Average CLIP Similarity: {average_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6a696e-b6d0-48c5-8ab7-107f766a091b",
   "metadata": {},
   "source": [
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e6aaba98-2651-4b08-87ea-60139dfce168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "Saving real images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:09<00:00, 50.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating and saving synthetic images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:12<00:00, 38.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# from pytorch_fid import calculate_fid_given_paths\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "from model import CVAE\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "# Directories for real and generated images\n",
    "real_images_dir = \"real_images\"\n",
    "generated_images_dir = \"generated_images\"\n",
    "os.makedirs(real_images_dir, exist_ok=True)\n",
    "os.makedirs(generated_images_dir, exist_ok=True)\n",
    "\n",
    "# Dataset and Model Setup\n",
    "test_annotations_file = \"autodl-tmp/annotations/captions_val2017.json\"\n",
    "test_coco = COCO(test_annotations_file)\n",
    "\n",
    "latent_dim = 128\n",
    "condition_dim = 512\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "subset_fraction = 0.1  # Smaller subset for FID calculation\n",
    "\n",
    "# Load CVAE Model\n",
    "model = CVAE(condition_dim=condition_dim, latent_dim=latent_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"model_250.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Load CLIP Model\n",
    "model_clip, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Transform for Real Images\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "# Extract a Subset of Real Images and Save Them\n",
    "image_ids = list(test_coco.imgs.keys())\n",
    "subset_size = int(len(image_ids) * subset_fraction)\n",
    "image_ids = image_ids[:subset_size]\n",
    "\n",
    "print(\"Saving real images...\")\n",
    "for i, img_id in enumerate(tqdm(image_ids)):\n",
    "    image_info = test_coco.loadImgs(img_id)[0]\n",
    "    img_path = f\"autodl-tmp/val2017/{image_info['file_name']}\"  # Update to your dataset path\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img = transform(img)\n",
    "    save_image(img, os.path.join(real_images_dir, f\"{i}.png\"))\n",
    "\n",
    "# Generate Synthetic Images and Save Them\n",
    "test_captions = []\n",
    "for img_id in image_ids:\n",
    "    ann_ids = test_coco.getAnnIds(imgIds=img_id)\n",
    "    annotations = test_coco.loadAnns(ann_ids)\n",
    "    caption = annotations[0]['caption']\n",
    "    test_captions.append(caption)\n",
    "\n",
    "print(\"Generating and saving synthetic images...\")\n",
    "for i, caption in enumerate(tqdm(test_captions)):\n",
    "    with torch.no_grad():\n",
    "        text_token = clip.tokenize([caption]).to(device)\n",
    "        text_feature = model_clip.encode_text(text_token)\n",
    "        z = torch.randn(1, latent_dim).to(device)  # Sample a latent vector\n",
    "        generated_image = model.decoder(z, text_feature).squeeze(0).cpu()\n",
    "        save_image(generated_image, os.path.join(generated_images_dir, f\"{i}.png\"))\n",
    "\n",
    "# # Calculate FID\n",
    "# print(\"Calculating FID...\")\n",
    "# fid_score = calculate_fid_given_paths(\n",
    "#     [real_images_dir, generated_images_dir],\n",
    "#     batch_size=50,\n",
    "#     device=device,\n",
    "#     dims=2048,  # Default dimension used in InceptionV3\n",
    "# )\n",
    "# print(f\"FID Score: {fid_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cb9ea35-64ed-4303-a4d0-462f3eacf428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__']\n"
     ]
    }
   ],
   "source": [
    "import pytorch_fid\n",
    "print(dir(pytorch_fid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
